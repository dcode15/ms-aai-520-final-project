version: '3'

x-common-params: &common-params
  build: .
  volumes:
    - ./data:/usr/llm/data
    - ./src:/usr/llm/src
    - model-cache:/model_cache
  tty: true
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [ gpu ]
  environment:
    - OPENAI_API_KEY=${OPENAI_API_KEY}
    - HF_TOKEN=${HF_TOKEN}

services:
  fine-tune:
    <<: *common-params
    entrypoint: ["python", "fine_tune_model.py"]
  inference:
    <<: *common-params
    entrypoint: [ "python", "inference.py" ]
  generate-rlaif-data:
    <<: *common-params
    entrypoint: [ "python", "generate_rlhf_data.py" ]
  train-reward-model:
    <<: *common-params
    entrypoint: [ "python", "train_reward_model.py" ]
  rlaif:
    <<: *common-params
    entrypoint: [ "python", "rlhf.py" ]




volumes:
  model-cache: