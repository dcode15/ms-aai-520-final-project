x-common-params: &common-params
  build: .
  volumes:
    - ./data:/usr/llm/data
    - ./src:/usr/llm/src
    - model-cache:/model_cache
  tty: true
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [ gpu ]
  environment:
    - OPENAI_API_KEY=${OPENAI_API_KEY}
    - HF_TOKEN=${HF_TOKEN}

services:
  fine-tune:
    <<: *common-params
    entrypoint: [ "python", "fine_tune_model.py" ]
  inference:
    <<: *common-params
    entrypoint: [ "python", "inference.py" ]
  generate-dpo-data:
    <<: *common-params
    entrypoint: [ "python", "generate_dpo_data.py" ]
  dpo:
    <<: *common-params
    entrypoint: [ "python", "dpo.py" ]
  evaluate-model:
    <<: *common-params
    entrypoint: ["python", "evaluate_model.py"]
  generate-test-responses:
    <<: *common-params
    entrypoint: [ "python", "generate_test_responses.py" ]


volumes:
  model-cache: