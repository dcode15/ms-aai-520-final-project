\documentclass[stu,donotrepeattitle,floatsintext]{apa7}

\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage[style=apa,backend=biber]{biblatex}
\usepackage[options]{nohyperref}
\usepackage{url}
\usepackage{appendix}

\DeclareLanguageMapping{american}{american-apa}
\addbibresource{main.bib}

\titleformat{\section}
{\normalfont\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\bfseries}{\thesubsubsection}{1em}{}

\newcommand{\q}[1]{``#1''}
\newcommand{\customsection}[2]{
  \phantomsection
  \section*{#1}\label{#2}
  \addcontentsline{toc}{section}{#1}
}
\newcommand{\customsubsection}[2]{
  \phantomsection
  \subsection*{#1}\label{#2}
  \addcontentsline{toc}{subsection}{#1}
}
\newcommand{\customsubsubsection}[2]{
  \phantomsection
  \subsubsection*{#1}\label{#2}
  \addcontentsline{toc}{subsubsection}{#1}
}

\title{Developing a Memory-Efficient Dialogue Chatbot}

\authorsnames{Douglas Code}
\authorsaffiliations{University of San Diego}
\course{AAI-520: Natural Language Processing and Generative AI}
\professor{Andrew Van Benschoten, Ph.D}
\duedate{Octboer 21, 2024}

\begin{document}
    \maketitle

    \tableofcontents
    \newpage


    \customsection{Introduction}{introduction}

    The goal of this project was to fine-tune a small, pretrained large language model (LLM) to generate realistic and compelling movie dialogue when prompted with a preceding line.
    To achieve this, the Qwen 2.5 3B base model was fine-tuned using the Cornell Movie-Dialog Corpus, a curated dataset of movie dialogue.

    The project consisted of multiple stages, including model selection, data preprocessing, fine-tuning, and Direct Preference Optimization (DPO).
    A web UI and API were then developed to make the model easily accessible.
    The resulting model achieved significant improvement over the base model in dialogue generation, with human testers preferring output from the fine-tuned and DPO versions of the model 89\% of the time when compared with the base model.

    \customsection{Model Architecture}{architecture}

    \customsubsection{Model Selection}{model_selection}

    For this project, the Qwen 2.5 3B base model was selected as the base model.
    This model was released in September 2024 and is part of the broader Qwen series of models developed by the Alibaba Group~\parencite{qwen_2.5}.

    A number of factors contributed to the selection of this model.
    One of the largest constraints in developing the dialogue model was in the amount of GPU compute and VRAM available.
    All model training was done on a single NVIDIA RTX 3060ti with 8 gigabytes of VRAM.
    This greatly limited the size of model that could be used, making the Qwen series appealing due to its inclusion of 0.5 billion, 1.5 billion, and 3 billion parameter models.
    This allowed early work to be done on the small 0.5B model, then as efficiency improvements were made in the training process the model could be retrained using the 1.5B and finally 3B base models.

    Additionally, the small models in the Qwen 2.5 series represent the current state of the art in small open-source pretrained models, currently ranking first in the Hugging Face Open LLM Leaderboards in terms of average score across all metrics~\parencite{huggingface_leaderboard}.
    In initial testing in comparison to other options like Meta's Llama 3.2 and Google's Gemma models, the Qwen 2.5 models particularly excelled in both creativity and the lower strength of their content safeguards.
    While weaker safeguards would be a liability in many situations, the nature of movie dialogue generation requires the in-grained \q{assistant} role to be overridden, and also introduces a need to handle some content that might be refused by most prominent pretrained base models.

    \customsubsection{Preprocessing}{preprocesssing}

    Preprocessing the data for use in fine-tuning involved cross-referencing the movie lines data and movie conversations data to create ordered lists of lines representing full conversations.
    These conversations were then split into prompt-response pairs using a sliding window, where the first and second lines would form the first pair, and the second and third lines would form the second pair, etc.
    Finally, the paired lines were formatted using the Qwen models' special tokens to match the formatting expected by the model:

    \begin{verbatim}
        <|im_start|>user
        [Prompt line]<|im_end|>
        <|im_start|>assistant
        [Response line]<|im_end|><|endoftext|>
    \end{verbatim}

    Formatted sequences were then truncated at a max length of 512 tokens, which was chosen to balance preserving context with remaining compuatationally efficient on the available hardware.
    To more efficiently use GPU resources during fine-tuning, packing was used.
    Packing involves concatenating short sequences together into a single training example to more fully fill the maximum sequence length.
    This prevents the situation where a long sequence of padding tokens need to be processed, wasting GPU resources~\parencite{packing}.

    \customsubsection{Fine-Tuning}{fine_tuning}

    \customsubsection{Direct Preference Optimization}{dpo}

    \customsection{Challenges and Solutions}{challenges}

    \customsubsection{GPU Optimization}{gpu_optimization}

    \customsubsubsection{QLoRA}{qlora}

    \customsubsubsection{Mixed Precision Training}{mixed_precision}

    \customsubsubsection{Flash Attention}{flash_attention}

    \customsubsubsection{8-bit Optimizer}{8bit_optimizer}

    \customsubsubsection{Gradient Accumulation}{gradient_accumulation}

    \customsubsubsection{Inference Sequence Length}{inference_length}

    \customsubsection{RLAIF}{rlaif}

    \customsubsection{Web Interface}{web_interface}

    \customsubsubsection{API}{api}

    \customsubsubsection{UI}{UI}

    \customsection{Evaluation}{evaluation}

    \customsubsection{Automated Metrics}{automated_metrics}

    \customsubsection{Human Evaluation}{human_evaluation}

    \customsection{Future Improvements and Scalability}{future_improvements}

    \customsubsection{Model Improvements}{model_improvements}

    \customsubsubsection{Larger Base Models}{larger_base_models}

    \customsubsubsection{Expanded and Genre-Specific Datasets}{expanded_datasets}

    \customsubsubsection{Human Tuning}{human_tuning}

    \customsubsubsection{Hyperparameter Tuning}{hyperparameter_tuning}

    \customsubsubsection{Safety Tuning}{safety_tuning}

    \customsubsection{Scalability}

    \customsubsubsection{Model Distillation}{model_distillation}

    \customsubsubsection{Cloud Architecture}{cloud_architecture}

    \printbibliography

    \begin{appendices}
        \section{}\label{sec:}


    \end{appendices}

\end{document}