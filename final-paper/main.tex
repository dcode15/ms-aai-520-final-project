\documentclass[stu,donotrepeattitle,floatsintext]{apa7}

\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage[style=apa,backend=biber]{biblatex}
\usepackage[options]{nohyperref}
\usepackage{url}
\usepackage{appendix}

\DeclareLanguageMapping{american}{american-apa}
\addbibresource{main.bib}

\titleformat{\section}
{\normalfont\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\normalfont\bfseries}{\thesubsubsection}{1em}{}

\newcommand{\q}[1]{``#1''}
\newcommand{\customsection}[2]{
  \phantomsection
  \section*{#1}\label{#2}
  \addcontentsline{toc}{section}{#1}
}
\newcommand{\customsubsection}[2]{
  \phantomsection
  \subsection*{#1}\label{#2}
  \addcontentsline{toc}{subsection}{#1}
}
\newcommand{\customsubsubsection}[2]{
  \phantomsection
  \subsubsection*{#1}\label{#2}
  \addcontentsline{toc}{subsubsection}{#1}
}

\title{Developing a Memory-Efficient Dialogue Chatbot}

\authorsnames{Douglas Code}
\authorsaffiliations{University of San Diego}
\course{AAI-520: Natural Language Processing and Generative AI}
\professor{Andrew Van Benschoten, Ph.D}
\duedate{Octboer 21, 2024}

\begin{document}
    \maketitle

    \tableofcontents
    \newpage


    \customsection{Introduction}{introduction}

    The goal of this project was to fine-tune a small, pretrained large language model (LLM) to generate realistic and compelling movie dialogue when prompted with a preceding line.
    To achieve this, the Qwen 2.5 3B base model was fine-tuned using the Cornell Movie-Dialog Corpus, a curated dataset of movie dialogue~\parencite{cornell_movie_data}.

    The project consisted of multiple stages, including model selection, data preprocessing, fine-tuning, and Direct Preference Optimization (DPO).
    A web UI and API were then developed to make the model easily accessible.
    The resulting model achieved significant improvement over the base model in dialogue generation, with human testers preferring output from the fine-tuned and DPO versions of the model 89\% of the time when compared with the base model.

    \customsection{Model Selection}{model_selection}

    For this project, the Qwen 2.5 3B base model was selected as the base model.
    This model was released in September 2024 and is part of the broader Qwen series of models developed by the Alibaba Group~\parencite{qwen_2.5}.

    A number of factors contributed to the selection of this model.
    One of the largest constraints in developing the dialogue model was in the amount of GPU compute and VRAM available.
    All model training was done on a single NVIDIA RTX 3060ti with 8 gigabytes of VRAM.
    This greatly limited the size of model that could be used, making the Qwen series appealing due to its inclusion of 0.5 billion, 1.5 billion, and 3 billion parameter models.
    This allowed early work to be done on the small 0.5B model, then as efficiency improvements were made in the training process the model could be retrained using the 1.5B and finally 3B base models.

    Additionally, the small models in the Qwen 2.5 series represent the current state of the art in small open-source pretrained models, currently ranking first in the Hugging Face Open LLM Leaderboards in terms of average score across all metrics~\parencite{huggingface_leaderboard}.
    In initial testing in comparison to other options like Meta's Llama 3.2 and Google's Gemma models, the Qwen 2.5 models particularly excelled in both creativity and the lower strength of their content safeguards.
    While weaker safeguards would be a liability in many situations, the nature of movie dialogue generation requires the in-grained \q{assistant} role to be overridden, and also introduces a need to handle some content that might be refused by most prominent pretrained base models.

    \customsection{Preprocessing}{preprocesssing}

    Preprocessing the data for use in fine-tuning involved cross-referencing the movie lines data and movie conversations data to create ordered lists of lines representing full conversations.
    These conversations were then split into prompt-response pairs using a sliding window, where the first and second lines would form the first pair, and the second and third lines would form the second pair, etc.
    Finally, the paired lines were formatted using the Qwen models' special tokens to match the formatting expected by the model:
    \begin{verbatim}
        <|im_start|>user
        [Prompt text]<|im_end|>
        <|im_start|>assistant
        [Response text]<|im_end|><|endoftext|>
    \end{verbatim}
    Formatted sequences were then truncated at a max length of 512 tokens, which was chosen to balance preserving context with remaining compuatationally efficient on the available hardware.
    To more efficiently use GPU resources during fine-tuning, packing was used.
    Packing involves concatenating short sequences together into a single training example to more fully fill the maximum sequence length.
    This prevents the situation where a long sequence of padding tokens need to be processed, wasting GPU resources~\parencite{packing}.

    The preprocessed data was split into training and test datasets using an 80/20 split, and 20\% of the training dataset was held out validation.

    \customsection{Fine-Tuning}{fine_tuning}

    The largest challenge in fine-tuning the model was in the limited computational resources that were available.
    In particular, the 8GB of VRAM presented an issue for loading models for training.
    This combined with the short timeline available for training created a balancing act in trying to use the largest, most powerful model possible while also keeping all values on the GPU during training.
    To address this, a number of optimizations were made to reduce the amount of VRAM and compute needed during fine-tuning.
    In combination with each other, these techniques allowed the 500 million parameter Qwen 2.5 model to be increased to the 3 billion parameter model while also significantly reducing training time.
    While some of the techniques used can result in decreased model quality, testing showed that the effect of being able to increase the model size significantly outweighed any degradation in model performance.

    \customsubsection{Quantization}{quantization}

    Quantization involves converting a model's parameters to lower-precision formats, reducing the amount of memory and compute needed.
    4-bit quantization was used in this case, converting the 32-bit floating point parameters into 4-bit normal floating point format.
    For intermediate calculations, 16-bit brain floating point format was used.

    \customsubsection{QLoRA}{qlora}

    Low-Rank Adaptation (LoRA) is a technique where model weights are frozen during training and small rank decomposition matrices are added to the model's layers.
    These matrices are then updated during fine-tuning rather than the model weights, allowing larger models to be fine-tuned with relatively few trainable parameters.
    The decreased number of parameters both speeds up training and decreases amount of memory used.
    The use of 4-bit quantization allowed for the use of Quantized Low-Rank Adaptation (QLoRA).
    QLoRA quantizes the base model to 4 bits and trains the LoRA adapters in 16-bit precision.
    Then during inference, quantized model weights are dequantized as needed and combined with LoRA updates to get a final result~\parencite{qlora}.

    \customsubsection{Flash Attention}{flash_attention}
    Flash Attention is an algorithm for efficiently performing attention computations in transformer models.
    It works by optimizing the usage of shared random-access memory (SRAM) as opposed to much slower high-bandwidth memory (HBM), resulting in faster computations and decreased memory usage.
    An updated version of the Flash Attention algorithm, Flash Attention 2, was used for this project~\parencite{flash_attention}.

    \customsubsection{Gradient Accumulation}{gradient_accumulation}
    Gradient accumulation is a technique where gradients are summed over multiple mini-batches, and then parameters are only updated after a set number of mini-batches rather than after every batch.
    This can allow the use of larger simulated batch sizes than are otherwise possible due to memory limitations~\parencite{gradient_accumulation}.
    During fine-tuning, batch sizes of 1 were used with gradient accumulation to achieve simulated batch sizes of 16 sequences.

    \customsubsection{NEFTune}{neftune}

    Noise-Enhanced Fine-Tuning (NEFTune) adds small amounts of Gaussian noise to the input embeddings.
    The addition of noise acts as a form of regularization, similar to how noise might be added to training images in image recognition tasks.
    This can prevent overfitting and improve model generalization~\parencite{neftune}.

    \customsection{Direct Preference Optimization}{dpo}

    After the base model was fine-tuned using the movie dialogue, reinforcement learning from AI feedback (RLAIF) was performed to further fine-tune the model.
    RLAIF was chosen over RLHF due to RLHF not being feasible given the time and resource constraints of the project.
    To achieve this, 10,000 example dialogue prompts were generated using the GPT-4o model through the OpenAI API .
    For each of these dialogue prompts, two competing responses were then generated by the fine-tuned model.
    Each prompt and its response options were then fed back into the GPT-4o model, which was prompted to \q{select the response that is most coherent and works best as dialogue from a movie}.

    Once 10,000 preference examples had been created, a reward model was trained on the preference model using the Qwen 2.5 0.5B model as a base.
    The reward model was then used for Proximal Policy Optimization (PPO).
    However, the multiple steps of training the reward model made it very time-consuming to make updates to the model, and the process was very sensitive to the quality of the reward model.

    To address this the approach of using RLAIF with PPO was abandoned and Direct Preference Optimization (DPO) was used instead.
    DPO allows language models to be tuned on preference data directly without creating a reward model~\parencite{dpo}.
    This allowed the final model to be trained much more efficiently by training the fine-tuned model on the existing preference data without any intermediate steps.

    \customsection{Web Interface}{web_interface}

    To facilitate easy interaction with the model, a chatbot-style web interface was created.
    The web interface uses a containerized FastAPI API which performs inference when provided with a prompt.
    The API is consumed by a Vue-based web UI that allows the user to send messages, start new conversations, and modify key inference parameters like Temperature, Top K filtering, and Top P filtering.

    \customsubsection{Inference Sequence Length}{inference_length}

    The unique nature of the dialogue generation task presented an opportunity for improving inference speed through limiting the maximum number of tokens generated.
    Aside from monologues, it is rare for any single \q{turn} in a conversation to last for more than a few sentences.
    This meant that the length of generated sequences could be greatly reduced without noticeably impacting the quality of the model.
    For the web interface, chatbot responses are limited to a maximum of 64 tokens, which greatly reduces API times.

    \customsection{Evaluation}{evaluation}

    \customsubsection{Automated Metrics}{automated_metrics}

    \customsubsection{Human Evaluation}{human_evaluation}

    This model's creative nature made automated evaluation metrics less useful than for more common LLM tasks like summarization or machine translation.
    To get a more comprehensive picture of the final model's performance, human evaluation was performed as well.
    This was done by giving two individuals 100 prompts from the test dataset along with responses from the base Qwen 2.5 model, fine-tuned model, and fine-tuned model with DPO.
    The individuals then chose the response they thought was most coherent and interesting in the context of movie dialogue.
    To better evaluate the Qwen 2.5 base model's ability to perform the task, it was given the system prompt: \q{Respond to all prompts with realistic and natural dialogue lines}.

    Human evaluation showed strong preference for the tuned models, with the final model being chosen 52\% of the time, the fine-tuned-only model being chosen 37\% of the time, and the base model being chosen 11\% of the time.
    Subjectively, evaluators noted that the fine-tuned model sometimes showed more creativity in creating interesting responses than the final model that had undergone DPO, but the DPO model was more consistently coherent and less likely to create repetitive, rambling answers.

    \customsection{Future Improvements and Scalability}{future_improvements}

    Given additional time and resources, there are a number of improvements that could be made to both the model itself and its ability to scale.

    \customsubsection{Model Improvements}{model_improvements}

    \customsubsubsection{Larger Base Models}{larger_base_models}

    With more GPU resources, larger base models could be used.
    While the capabilities of the small Qwen 2.5 models are impressive for their size, there is still a very noticeable difference in output quality when compared to larger models.
    Performing a similar training process on the 7B, 14B, 32B, or 72B models would be one of the simplest ways to make large improvements to the quality of the model.

    \customsubsubsection{Expanded and Genre-Specific Datasets}{expanded_datasets}

    Both the vocabulary and style of dialogue changes significantly over time and between genres.
    With the Cornell Movie Dialog corpus being six years old now, the model could be improved by incorporating additional data from more recent movies.
    Tuning on datasets representing different genres could also give the model better a better ability to handle more diverse prompt topics.

    \customsubsubsection{Human Tuning}{human_tuning}

    In this project, preference data was generated using a more sophisticated large language model.
    While this is much more cost and time effective than collecting human preference data, it also aligned the model with the limitations and biases trained of the GPT-4o model, which isn't necessarily identical to human preference.
    Performing tuning based on actual human feedback could give a better alignment with human preference.

    \customsubsubsection{Hyperparameter Tuning}{hyperparameter_tuning}

    Even with the GPU optimizations described previously, training a new iteration of the model took around two days end-to-end.
    This made it infeasible to perform hyperparameter tuning beyond manually tweaking a few key parameters like DPO's beta parameter.
    With more GPU resources and the ability to tune multiple models at once across a distributed cluster, more thorough hyperparameter tuning could be done, which would likely result in significant improvements to model quality.

    \customsubsubsection{Model Safety}{model_safety}

    The fine-tuning process largely eliminated the content safeguards of the base model.
    While this is desirable for this task in many situations, as movies often deal with sensitive topics, further research into what the tuned model will and won't do would give a better idea of potential issues.
    The reintroduction of some safeguards would also present the opportunity to create multiple versions of the model with different levels of restriction.
    This could make it more accessible to a wider range of users, like children.

    \customsubsubsection{Scaling}{scaling}

    Currently, the model is served using a \q{serverless} architecture that scales down to zero running instances when not in use.
    To achieve better response times while still maintaining some level of automated scaling, a persistent server or cluster could be run to accommodate baseline traffic with additional auto-scaling or serverless compute to handle spikes in traffic.
    In a future iteration of the chatbot that includes features like API access or user-driven system prompts that can lead to regularly repeated sequence prefixes, prompt caching could be used to save heavily re-used transformer block calculations to decrease computational needs.
    Model pruning could also potentially be used to remove model weights that contribute little to the model's performance.
    This would decrease the model size, allowing for faster inference and less RAM usage.

    \printbibliography

    \begin{appendices}
        \section{}\label{sec:}


    \end{appendices}

\end{document}