\documentclass[stu,donotrepeattitle,floatsintext]{apa7}

\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage[style=apa,backend=biber]{biblatex}
\usepackage[options]{nohyperref}
\usepackage{url}
\usepackage{appendix}

\DeclareLanguageMapping{american}{american-apa}
\addbibresource{main.bib}
\linespread{1.5}


\newcommand{\q}[1]{``#1''}
\newcommand{\customsection}[2]{
  \phantomsection
  \section*{#1}\label{#2}
  \addcontentsline{toc}{section}{#1}
}
\newcommand{\customsubsection}[2]{
  \phantomsection
  \subsection*{#1}\label{#2}
  \addcontentsline{toc}{subsection}{#1}
}
\newcommand{\customsubsubsection}[2]{
  \phantomsection
  \subsubsection*{#1}\label{#2}
  \addcontentsline{toc}{subsubsection}{#1}
}

\title{Building a Memory-Efficient Dialogue Chatbot}

\authorsnames{Douglas Code}
\authorsaffiliations{University of San Diego}
\course{AAI-520: Natural Language Processing and Generative AI}
\professor{Andrew Van Benschoten, Ph.D}
\duedate{Octboer 21, 2024}

\begin{document}
    \maketitle

    \tableofcontents
    \newpage


    \customsection{Introduction}{introduction}

    The goal of this project was to fine-tune a small, pretrained large language model (LLM) to generate realistic and compelling movie dialogue when prompted with a preceding line.
    To achieve this, the Qwen 2.5 3B base model was fine-tuned using the Cornell Movie-Dialog Corpus, a curated dataset of movie dialogue~\parencite{cornell_movie_data}.

    \customsection{Model Selection}{model_selection}

    For this project, the Qwen 2.5 3B base model was selected as the base model.
    This model was released in September 2024 and is part of the broader Qwen series of models developed by the Alibaba Group~\parencite{qwen}.

    One of the largest constraints in developing the dialogue model was in the amount of GPU compute and VRAM available, as all model training was done on a single NVIDIA RTX 3060ti with 8 gigabytes of VRAM .
    This greatly limited the size of model that could be used, making the Qwen series appealing due to its inclusion of 0.5 billion, 1.5 billion, and 3 billion parameter models.
    This allowed early work to be done on the small 0.5B model, then as efficiency improvements were made in the training process the model could be retrained using the 1.5B and finally 3B base models.

    Additionally, the small models in the Qwen 2.5 series represent the current state of the art in small open-source pretrained base models, currently ranking first in the Hugging Face Open LLM Leaderboards in terms of average score across all metrics~\parencite{huggingface_leaderboard}.
    In initial testing in comparison to other options like Meta's Llama 3.2 and Google's Gemma models, the Qwen 2.5 models particularly excelled in their creative output.

    \customsection{Preprocessing}{preprocesssing}

    Preprocessing the data for use in fine-tuning involved cross-referencing the movie lines data and movie conversations data to create ordered lists of lines representing full conversations.
    These conversations were then split into prompt-response pairs using a sliding window, where the first and second lines would form the first pair, the second and third lines would form the second pair, etc.
    Finally, the paired lines were formatted using the Qwen models' special tokens to match the formatting expected by the model.

    Formatted sequences were truncated at a max length of 512 tokens, which was chosen to balance preserving context with remaining computationally efficient on the available hardware.
    To more efficiently use GPU resources during fine-tuning, packing was used.
    Packing involves concatenating short sequences together into a single training example to more fully fill the maximum sequence length.
    This prevents the situation where a long sequence of padding tokens need to be processed, wasting GPU resources~\parencite{packing}.

    The preprocessed data was split into training and test datasets using an 80/20 split, and 20\% of the training dataset was held out for validation.

    \customsection{Fine-Tuning}{fine_tuning}

    The largest challenge in fine-tuning the model was in the limited computational resources that were available.
    In particular, the 8GB of VRAM presented an issue for loading models for training.
    This combined with the short timeline available for training created a balancing act in trying to use the largest, most powerful model possible while also keeping all values on the GPU during training.
    To address this, a number of optimizations were made to reduce the amount of VRAM and compute needed during fine-tuning.
    In combination with each other, these techniques allowed the 500 million parameter Qwen 2.5 model to be increased to the 3 billion parameter model while also significantly reducing training time.
    While some of the techniques used can result in decreased model quality, testing showed that the effect of being able to increase the model size significantly outweighed any degradation in model performance.

    \customsubsection{Quantization}{quantization}

    Quantization involves converting a model's parameters to lower-precision formats, reducing the amount of memory and compute needed.
    4-bit quantization was used in this case, converting the 32-bit floating point parameters into 4-bit normal floating point format.
    For intermediate calculations, 16-bit brain floating point format was used.

    \customsubsection{QLoRA}{qlora}

    Low-Rank Adaptation (LoRA), or Quantized Low-Rank Adaptation (QLoRA) when using 4-bit quantization, is a technique where model weights are frozen during training and small rank decomposition matrices are added to the model's layers.
    These matrices are then updated during fine-tuning rather than the model weights, allowing larger models to be fine-tuned with relatively few trainable parameters~\parencite{qlora}.

    \customsubsection{Flash Attention}{flash_attention}
    Flash Attention is an algorithm for efficiently performing attention computations in transformer models.
    It works by optimizing the usage of shared random-access memory (SRAM) as opposed to much slower high-bandwidth memory (HBM), resulting in faster computations and decreased memory usage~\parencite{flash_attention}.

    \customsubsection{Gradient Accumulation}{gradient_accumulation}
    Gradient accumulation is a technique where gradients are summed over multiple mini-batches, and parameters are only updated after a set number of mini-batches rather than after every batch.
    This can allow the use of larger simulated batch sizes than are otherwise possible due to memory limitations~\parencite{gradient_accumulation}.

    \customsubsection{NEFTune}{neftune}

    Noise-Enhanced Fine-Tuning (NEFTune) adds small amounts of Gaussian noise to the input embeddings.
    The addition of this noise acts as a form of regularization, improving model generalization by preventing overfitting~\parencite{neftune}.

    \customsection{Direct Preference Optimization}{dpo}

    After the base model was fine-tuned using the movie dialogue, reinforcement learning from AI feedback (RLAIF) was performed to further fine-tune the model.
    RLAIF was chosen over RLHF due to RLHF not being feasible given the time and resource constraints of the project.
    To achieve this, 10,000 example dialogue prompts were generated using the GPT-4o model through the OpenAI API .
    For each of these dialogue prompts, two competing responses were then generated by the fine-tuned model.
    Each prompt and its response options were then fed back into the GPT-4o model, which was prompted to choose a preferred option.
    Examples of the generated preference data can be found in Appendix~\ref{sec:example-dpo-data}.

    Once 10,000 preference examples had been created, a reward model for Proximal Policy Optimization (PPO) was trained on the preference model using the Qwen 2.5 0.5B model as a base.
    However, the multiple steps of training the reward model made it very time-consuming to make updates to the model, and the process was highly sensitive to the quality of the reward model.

    To address this, the approach of using RLAIF with PPO was altered to use Direct Preference Optimization (DPO) instead.
    DPO allows language models to be tuned on preference data directly without creating a reward model~\parencite{dpo}.
    This allowed the final model to be trained much more efficiently by training the fine-tuned model on the existing preference data without any intermediate steps.

    \customsection{Web Interface}{web_interface}

    To facilitate easy interaction with the model, a chatbot-style web interface was created.
    The web interface uses a containerized FastAPI API which performs inference when provided with a prompt.
    The API is consumed by a Vue-based web UI that allows the user to send messages, start new conversations, and modify key inference parameters like Temperature, Top K filtering, and Top P filtering.
    The URL for the web interface can be found in Appendix~\ref{sec:web-interface-link}.

    \customsubsection{Inference Sequence Length}{inference_length}

    Aside from monologues, it is rare for any single turn in a conversation to last for more than a few sentences.
    This meant that the length of generated sequences could be greatly reduced to a maximum of 64 tokens without noticeably impacting the usefulness of the chatbot.
    Limiting the generated sequence length in this way significantly improved chatbot response times.

    \customsection{Evaluation}{evaluation}

    This model's creative nature made many common evaluation metrics like BLEU and ROUGE less useful than for more common LLM tasks like summarization or machine translation.
    To address this, human evaluation was combined with G-Eval, an automated LLM-based technique that has shown much greater alignment with human preference than BLEU or ROUGE in dialogue generation tasks~\parencite{g_eval}.

    \customsubsection{G-Eval}{g_eval}

    G-Eval uses automatic chain-of-\thought prompting to score responses across a set of criteria.
    To do this, initial prompts were created for four criteria (coherence, consistency, fluency, relevance) that defined the task and the evaluation criteria.
    This initial prompt was then passed into the GPT-4o large language model for it to expand the prompt with chain-of-thought steps for assigning a score between one and 5 for a prompt-response pair.
    The full set of final prompts can be seen in Appendix~\ref{sec:g-eval-prompts}.

    Each of the three model iterations (base, fine-tuned, and fine-tuned with DPO) were then used to generate a response to 2,000 prompts from the test dataset.
    For each of the four criteria, the resulting 6,000 responses were then passed into the GPT-4o Mini model along with the corresponding criteria prompt to obtain a score, resulting in 24,000 total evaluations.
    The use of GPT-4o models to perform both the DPO tuning and evaluation presents a possible weakness in this methodology due to possible over-inflation of evaluation scores due to alignment between the two processes.
    To address this, Claude Sonnet 3.5 was initially used for the G-Eval evaluations, but was found to be too slow and expensive to be feasible for the volume of data, prompting the switch to GPT-4o Mini.

    To obtain a more nuanced evaluation score than a simple 1--5 scoring, the token log-probabilities from the evaluation responses were used to find the model's relative likelihood of selecting each score.
    These log-probabilities were then used to weight each score and arrive at a final adjusted score, meaning that a high-confidence 5 would score higher than a 5 that was barely selected over a 4.

    \customsubsection{Human Evaluation}{human_evaluation}

    Human evaluation was performed by giving two individuals 100 prompts from the test dataset along with responses from the base Qwen 2.5 model, fine-tuned model, and fine-tuned model with DPO .
    The individuals then chose the response they thought was most coherent and interesting in the context of movie dialogue.
    To better evaluate the Qwen 2.5 base model's ability to perform the task, it was given the system prompt: \q{Respond to all prompts with realistic and natural dialogue lines}.

    Human evaluation showed strong preference for the tuned models, with the final model with DPO being chosen 52\% of the time, the fine-tuned-only model being chosen 37\% of the time, and the base model being chosen 11\% of the time.

    \customsubsection{Results}{eval_results}

    Figure~\ref{fig:eval-metrics} shows the average score (scaled from zero to one) of each model across all G-Eval metrics, as well as the preference proportions from the head-to-head human evaluations.
    These results show consistent improvements across all metrics between the base model and fine-tuned model, as well as between the fine-tuned model and DPO model.
    This indicates that both the fine-tuning process and DPO tuning were able to provide significant boosts to model quality.

    \begin{figure}[tb]
        \centering
        \includegraphics[width=1\textwidth]{figures/eval-metrics-light}
        \caption{Evaluation scores across all metrics.}
        \label{fig:eval-metrics}
    \end{figure}

    Examining the score distributions in Figure~\ref{fig:eval-boxplot} gives more insight into model performance.
    While both G-Eval and human evaluation found that the base model did sometimes provide responses on par with or better than the other models, it was also much more likely to give very poor answers.
    The fine-tuned model achieved median and top-quartile performance comparable to the DPO model across all metrics except consistency, but its average score was pulled down significantly by being much more likely to provide low-scoring responses than the DPO model.
    This suggests that while the DPO tuning process only resulted in slight improvements among high-quality answers, it resulted in large reductions in low-quality answers.

    \begin{figure}[tb]
        \centering
        \includegraphics[width=1\textwidth]{figures/eval-boxplot-light}
        \caption{G-Eval scores distributions.}
        \label{fig:eval-boxplot}
    \end{figure}

    \customsection{Future Improvements and Scalability}{future_improvements}

    Given additional time and resources, there are a number of improvements that could be made to both the model itself and its ability to scale.

    \customsubsection{More Data, More Paramaters}

    With the Cornell Movie Dialog corpus being six years old now, the model could be improved by incorporating additional data from more recent movies.
    Tuning on datasets representing different genres could also give the model better a better ability to handle more diverse prompt topics.

    With more GPU resources, using larger base models would be one of the simplest ways to make large improvements to the quality of the model.
    While the capabilities of the small Qwen 2.5 models are impressive for their size, there is still a very noticeable difference in output quality when compared to larger models.

    \customsubsection{Human Tuning}{human_tuning}

    In this project, preference data was generated using a more sophisticated large language model.
    While this is much more cost and time effective than collecting human preference data, it also aligned the model with the limitations and biases trained of the GPT-4o model, which isn't necessarily identical to human preference.
    Performing tuning based on actual human feedback could give a better alignment with human preference.

    \customsubsection{Hyperparameter Tuning}{hyperparameter_tuning}

    Even with the GPU optimizations described previously, training a new iteration of the model took around two days end-to-end.
    This made it infeasible to perform hyperparameter tuning beyond manually tweaking a few key parameters like the beta parameter for DPO .
    With more GPU resources and the ability to tune multiple models at once across a distributed cluster, more thorough hyperparameter tuning could be done, which would likely result in significant improvements to model quality.

    \customsubsection{Scaling}{scaling}

    Currently, the model is served using a \q{serverless} architecture that scales down to zero running instances when not in use.
    To achieve better response times while still maintaining some level of automated scaling, a persistent server or cluster could be run to accommodate baseline traffic with additional auto-scaling or serverless compute to handle spikes in traffic.

    In a future iteration of the chatbot that includes features like API access or user-driven system prompts that can lead to regularly repeated sequence prefixes, prompt caching could be used to save heavily re-used transformer block calculations to decrease computational needs.
    Model pruning could also potentially be used to remove model weights that contribute little to the model's performance.
    This would decrease the model size, allowing for faster inference and less RAM usage.

    \printbibliography
    \clearpage

    \begin{appendices}
        \section{Appendix A: Web Interface Link}\label{sec:web-interface-link}

        \noindent https://dcode15.github.io/ms-aai-520-final-project/

        \bigskip
        \noindent Note that the first chatbot response will take approximately two minutes to return, as the container needs to start up and load the three models.

        \section{Appendix B: Example DPO Data}\label{sec:example-dpo-data}

        \noindent\textbf{Prompt}: \q{What if I told you I was the one who made that decision? Would you be mad?}\\
        \noindent\textbf{Chosen}: \q{Yes, I would be very angry. I do not like to be taken advantage of.}\\
        \noindent\textbf{Rejected}: \q{I'm sorry, but I can't provide an appropriate response to that request.}\\

        \bigskip
        \noindent\textbf{Prompt}: \q{I can't believe we made it through all that. We're stronger than I thought.}\\
        \noindent\textbf{Chosen}: \q{And we can be stronger still. So what are we waiting for?}\\
        \noindent\textbf{Rejected}: \q{Yeah, but it was a really hard movie to watch. Did you enjoy it? Did I?}\\

        \bigskip
        \noindent\textbf{Prompt}: \q{Why do we keep pretending everything is okay when it's falling apart?}\\
        \noindent\textbf{Chosen}: \q{The world is a mess. We all are. That's why we need to keep moving forward.}\\
        \noindent\textbf{Rejected}: \q{We do it because we're programmed to do so. In a way, it can be a good thing. It's not all bad if you're careful.}\\

        \bigskip
        \noindent\textbf{Prompt}: \q{I can't keep living in the shadows. It's time to step into the light.}\\
        \noindent\textbf{Chosen}: \q{Are you serious?}\\
        \noindent\textbf{Rejected}: \q{Dragons, I am no longer hiding in your shadows . Im stepping out to confront your fears . Dragons have never been scared of shadows, only light .}

        \section{Appendix C: G-eval Prompts}\label{sec:g-eval-prompts}
        \subsection{Coherence}\label{subsec:g-eval-coherence}

        \noindent You will be given a line of movie dialogue and an AI-generated response.
        Your task is to rate the AI's response on coherence.

        \bigskip
        \noindent Evaluation Criteria:\\
        \noindent Coherence (1-5) - The degree to which the AI's response logically connects to the given dialogue line and maintains a plausible flow of conversation within a movie context.
        A score of 1 indicates a completely incoherent response that doesn't fit the scene at all, while a score of 5 indicates a perfectly coherent response that naturally continues the dialogue in a way that could believably appear in a movie script.

        \bigskip
        \noindent Evaluation Steps:
        \begin{enumerate}
            \item Read the given movie dialogue line carefully, considering its potential context, tone, and implied setting.
            \item Read the AI's response and assess how well it follows from the given line in a cinematic context.
            \item Consider whether the response maintains the implied tone, setting, and character dynamics of the original line.
            \item Evaluate how well the response could continue or advance a potential movie scene.
            \item Assess whether the response introduces any abrupt or illogical shifts that would be jarring in a film dialogue.
            \item Assign a score from 1 to 5 based on the overall coherence of the AI's response in the context of movie dialogue.
        \end{enumerate}

        \subsection{Consistency}\label{subsec:g-eval-consistency}

        \noindent You will be given a line of movie dialogue and an AI-generated response.
        Your task is to rate the AI's response on consistency.

        \bigskip
        \noindent Evaluation Criteria:\\
        \noindent Consistency (1-5) - The degree to which the AI's response maintains consistent information, tone, and character voice with the given dialogue line.
        A score of 1 indicates highly inconsistent responses with clear contradictions or tonal mismatches, while a score of 5 indicates perfectly consistent responses that maintain the established context and character voice.

        \bigskip
        \noindent Evaluation Steps:
        \begin{enumerate}
            \item Carefully read the given movie dialogue line, noting any implied information about the character, setting, or situation.
            \item Read the AI's response and check if it's consistent with the information, tone, and character voice implied by the original line.
            \item Look for any contradictions in facts, emotions, or character traits between the original line and the response.
            \item Assess whether the response maintains a consistent level of formality, emotion, or genre-appropriate language.
            \item Check if the response's tone and style are consistent with what one would expect in a movie dialogue continuation.
            \item Assign a score from 1 to 5 based on the overall consistency of the AI's response with the given movie dialogue line.
        \end{enumerate}

        \subsection{Fluency}\label{subsec:g-eval-fluency}

        \noindent You will be given a line of movie dialogue and an AI-generated response.
        Your task is to rate the AI's response on fluency.

        \bigskip
        \noindent Evaluation Criteria:\\
        \noindent Fluency (1-5) - The quality of the AI's language in terms of grammar, vocabulary, and natural flow, specifically in the context of movie dialogue.
        A score of 1 indicates poor fluency with many errors and unnatural language that would be jarring in a film, while a score of 5 indicates perfect fluency that sounds natural and believable as movie dialogue.

        \bigskip
        \noindent Evaluation Steps:
        \begin{enumerate}
            \item Read the AI's response carefully, focusing on the language quality in the context of movie dialogue.
            \item Check for any grammatical errors, including issues with verb tenses, subject-verb agreement, and sentence structure.
            \item Assess the vocabulary use, looking for appropriate word choice and variety that fits well in a movie script.
            \item Evaluate the natural flow of language, checking if it sounds like realistic spoken dialogue.
            \item Look for any awkward phrasings or unnatural expressions that would sound out of place in a film.
            \item Consider the appropriate use of idioms, colloquialisms, or character-specific language that enhances the dialogue's authenticity.
            \item Assign a score from 1 to 5 based on the overall fluency of the AI's response as movie dialogue.
        \end{enumerate}

        \subsection{Relevance}\label{subsec:g-eval-relevance}

        \noindent You will be given a line of movie dialogue and an AI-generated response.
        Your task is to rate the AI's response on relevance.

        \bigskip
        \noindent Evaluation Criteria:\\
        \noindent Relevance (1-5) - The degree to which the AI's response appropriately addresses or follows up on the given dialogue line in a way that makes sense for a movie scene.
        A score of 1 indicates a completely irrelevant response that doesn't fit the context of the dialogue at all, while a score of 5 indicates a highly relevant response that perfectly continues or responds to the given line in a cinematically appropriate way.

        \bigskip
        \noindent Evaluation Steps:
        \begin{enumerate}
            \item Carefully read the given movie dialogue line, identifying the main points, emotions, or subtext it conveys.
            \item Assess how directly the AI's response addresses or builds upon the given line in a way that makes sense for a movie scene.
            \item Check if the response acknowledges all important aspects of the original line, or if it misses any crucial points.
            \item Evaluate whether the response contributes to advancing a potential scene or character development in a relevant way.
            \item Assess if the response stays on topic or if it introduces irrelevant information that doesn't fit the implied movie context.
            \item Consider the depth and specificity of the AI's response in relation to the given line and its potential place in a larger movie narrative.
            \item Assign a score from 1 to 5 based on the overall relevance of the AI's response to the given movie dialogue line.
        \end{enumerate}

    \end{appendices}

\end{document}